{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<configparser.ConfigParser object at 0x1111b98a0>\n",
      "AKIAQSKCN4UZMDWRMUBF\n",
      "                    Param                   Value\n",
      "0        DWH_CLUSTER_TYPE             single-node\n",
      "1           DWH_NUM_NODES                       1\n",
      "2           DWH_NODE_TYPE               dc2.large\n",
      "3  DWH_CLUSTER_IDENTIFIER  covid-redshift-cluster\n",
      "4                  DWH_DB                covid-db\n",
      "5             DWH_DB_USER                 awsuser\n",
      "6         DWH_DB_PASSWORD             Passw0rd123\n",
      "7                DWH_PORT                    5439\n",
      "8       DWH_IAM_ROLE_NAME      redshift-s3-access\n"
     ]
    }
   ],
   "source": [
    "#interact with AWS resources\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd \n",
    "import configparser\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "#setup config\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('cluster.config'))\n",
    "\n",
    "print(config)\n",
    "print(config.get('AWS', 'KEY'))\n",
    "\n",
    "#bring the configuration from the config file and store in variables \n",
    "KEY                    = config.get('AWS', 'KEY')\n",
    "SECRET                 = config.get('AWS', 'SECRET')\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_DB                 = config.get('DWH', 'DWH_DB')\n",
    "DWH_DB_USER            = config.get('DWH', 'DWH_DB_USER')\n",
    "DWH_DB_PASSWORD        = config.get('DWH', 'DWH_DB_PASSWORD')\n",
    "DWH_PORT               = config.get('DWH', 'DWH_PORT')\n",
    "DWH_IAM_ROLE_NAME       = config.get('DWH', 'DWH_IAM_ROLE_NAME')\n",
    "DWH_CLUSTER_TYPE       = config.get('DWH', 'DWH_CLUSTER_TYPE')\n",
    "DWH_NUM_NODES          = config.get('DWH', 'DWH_NUM_NODES')\n",
    "DWH_NODE_TYPE          = config.get('DWH', 'DWH_NODE_TYPE')\n",
    "\n",
    "SCHEMA_NAME = 'covid-db'\n",
    "S3_Staging = 's3://akash-covid-project-1/output'\n",
    "S3_Bucket = 'akash-covid-project-1'\n",
    "S3_Region = 'us-east-1'\n",
    "S3_output_dir = 'output'\n",
    "\n",
    "#create a df out of the above variables\n",
    "config_df = pd.DataFrame({'Param':\n",
    "                   ['DWH_CLUSTER_TYPE', 'DWH_NUM_NODES', 'DWH_NODE_TYPE', 'DWH_CLUSTER_IDENTIFIER', 'DWH_DB', 'DWH_DB_USER', 'DWH_DB_PASSWORD', 'DWH_PORT', 'DWH_IAM_ROLE_NAME'],\n",
    "                   'Value':\n",
    "                   [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "                  })\n",
    "\n",
    "print(config_df)\n",
    "\n",
    "#connect to the EC2 instance\n",
    "ec2 = boto3.resource('ec2',region_name=\"us-east-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "\n",
    "#connect to the S3 bucket\n",
    "s3 = boto3.resource('s3', region_name=\"us-east-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "\n",
    "#connect to the Redshift cluster\n",
    "redshift = boto3.client('redshift', region_name=\"us-east-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "\n",
    "#connect to the IAM role\n",
    "iam = boto3.client('iam', region_name=\"us-east-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "\n",
    "#connect to Athena \n",
    "athena = boto3.client('athena', region_name=\"us-east-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3', region_name=\"us-east-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "s3_client.download_file(S3_Bucket, 'output/fact_final.csv', 'fact_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fips province_state country_region  confirmed  deaths  recovered  active  \\\n",
      "0  72.0    Puerto Rico             US        3.0     0.0        0.0     NaN   \n",
      "1  72.0    Puerto Rico             US        3.0     0.0        0.0     NaN   \n",
      "2  72.0    Puerto Rico             US        3.0     0.0        0.0     NaN   \n",
      "3  72.0    Puerto Rico             US        3.0     0.0        0.0     NaN   \n",
      "4  72.0    Puerto Rico             US        3.0     0.0        0.0     NaN   \n",
      "\n",
      "       date  positive  negative  pending  hospitalized   death   total  \n",
      "0  20210307  101327.0  305972.0      NaN           NaN  2059.0  407299  \n",
      "1  20210306  101327.0  305972.0      NaN           NaN  2059.0  407299  \n",
      "2  20210305  101066.0  305972.0      NaN           NaN  2056.0  407038  \n",
      "3  20210304  100867.0  305972.0      NaN           NaN  2053.0  406839  \n",
      "4  20210303  100765.0  305972.0      NaN           NaN  2048.0  406737  \n"
     ]
    }
   ],
   "source": [
    "fact_final = pd.read_csv('fact_final.csv')\n",
    "#remove the first column as it is not needed\n",
    "fact_final = fact_final.iloc[:, 1:]\n",
    "print(fact_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fact_final_sql = pd.io.sql.get_schema(fact_final.reset_index(), 'fact_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"fact_final\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" REAL,\n",
      "  \"province_state\" TEXT,\n",
      "  \"country_region\" TEXT,\n",
      "  \"confirmed\" REAL,\n",
      "  \"deaths\" REAL,\n",
      "  \"recovered\" REAL,\n",
      "  \"active\" REAL,\n",
      "  \"date\" INTEGER,\n",
      "  \"positive\" REAL,\n",
      "  \"negative\" REAL,\n",
      "  \"pending\" REAL,\n",
      "  \"hospitalized\" REAL,\n",
      "  \"death\" REAL,\n",
      "  \"total\" INTEGER\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(fact_final_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zv/n4b0w4hd759g3cfc4fcllmwh0000gn/T/ipykernel_41423/4000575419.py:6: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dim_region_final = pd.read_csv('dim_region_final.csv').iloc[1,:1]\n"
     ]
    }
   ],
   "source": [
    "#similarly download the other tables and generate their schemas and then execute the sql to create the tables in redshift\n",
    "s3_client.download_file(S3_Bucket, 'output/dim_region_final.csv', 'dim_region_final.csv')\n",
    "s3_client.download_file(S3_Bucket, 'output/dim_date.csv', 'dim_date.csv')\n",
    "s3_client.download_file(S3_Bucket, 'output/dim_hospital.csv', 'dim_hospital.csv')\n",
    "\n",
    "dim_region_final = pd.read_csv('dim_region_final.csv').iloc[1,:1]\n",
    "dim_date = pd.read_csv('dim_date.csv').iloc[1,:1]\n",
    "dim_hospital = pd.read_csv('dim_hospital.csv').iloc[1,:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dim_region_final.head())\n",
    "print(dim_date.head())\n",
    "print(dim_hospital.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate th sql for the above tables\n",
    "dim_region_final_sql = pd.io.sql.get_schema(dim_region_final.reset_index(), 'dim_region_final')\n",
    "dim_date_sql = pd.io.sql.get_schema(dim_date.reset_index(), 'dim_date')\n",
    "dim_hospital_sql = pd.io.sql.get_schema(dim_hospital.reset_index(), 'dim_hospital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting redshift_connector\n",
      "  Downloading redshift_connector-2.1.0-py3-none-any.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scramp<1.5.0,>=1.2.0 (from redshift_connector)\n",
      "  Downloading scramp-1.4.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from redshift_connector) (2022.7)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from redshift_connector) (4.11.1)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.9.201 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from redshift_connector) (1.34.62)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from redshift_connector) (2.28.1)\n",
      "Requirement already satisfied: lxml>=4.6.5 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from redshift_connector) (4.9.1)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.12.201 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from redshift_connector) (1.34.62)\n",
      "Requirement already satisfied: packaging in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from redshift_connector) (22.0)\n",
      "Requirement already satisfied: setuptools in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from redshift_connector) (69.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift_connector) (2.3.2.post1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift_connector) (2022.12.7)\n",
      "Collecting asn1crypto>=1.5.1 (from scramp<1.5.0,>=1.2.0->redshift_connector)\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/akashreddymaligireddy/miniconda3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.12.201->redshift_connector) (1.16.0)\n",
      "Downloading redshift_connector-2.1.0-py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.0/125.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scramp-1.4.4-py3-none-any.whl (13 kB)\n",
      "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: asn1crypto, scramp, redshift_connector\n",
      "Successfully installed asn1crypto-1.5.1 redshift_connector-2.1.0 scramp-1.4.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install redshift_connector\n",
    "print(dim_region_final_sql)\n",
    "print(dim_date_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArrayContentNotHomogenousError', 'ArrayContentNotSupportedError', 'ArrayDimensionsNotConsistentError', 'BINARY', 'Binary', 'ClientProtocolVersion', 'Connection', 'Cursor', 'DataError', 'DatabaseError', 'Date', 'DateFromTicks', 'DbApiParamstyle', 'DriverInfo', 'Error', 'IamHelper', 'IntegrityError', 'InterfaceError', 'InternalError', 'NotSupportedError', 'OperationalError', 'PGEnum', 'PGJson', 'PGJsonb', 'PGText', 'PGTsvector', 'PGVarchar', 'ProgrammingError', 'RedshiftOID', 'RedshiftProperty', 'Time', 'TimeFromTicks', 'Timestamp', 'TimestampFromTicks', 'Warning', 'connect', 'make_divider_block', 'mask_secure_info_in_props']\n"
     ]
    }
   ],
   "source": [
    "import redshift_connector\n",
    "\n",
    "methods = [method for method in dir(redshift_connector) if callable(getattr(redshift_connector, method))]\n",
    "print(methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::039330899250:role/redshift-s3-access\n"
     ]
    }
   ],
   "source": [
    "#we need the iam role arn to attach to the redshift cluster. to connect s3 and redshift. ARN is the unique identifier for the role\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cluster': {'ClusterIdentifier': 'covid-redshift-cluster', 'NodeType': 'dc2.large', 'ClusterStatus': 'creating', 'ClusterAvailabilityStatus': 'Modifying', 'MasterUsername': 'awsuser', 'DBName': 'covid-db', 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0557e607d10acd930', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'default', 'VpcId': 'vpc-030ca89833b123009', 'PreferredMaintenanceWindow': 'sun:06:30-sun:07:00', 'PendingModifiedValues': {'MasterUserPassword': '****'}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 1, 'PubliclyAccessible': True, 'Encrypted': False, 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::039330899250:role/redshift-s3-access', 'ApplyStatus': 'adding'}], 'MaintenanceTrackName': 'current', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2024, 3, 24, 6, 30, tzinfo=tzutc()), 'ClusterNamespaceArn': 'arn:aws:redshift:us-east-1:039330899250:namespace:ae489cde-7558-4560-b4bf-91707de9bec0', 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}, 'MultiAZ': 'Disabled'}, 'ResponseMetadata': {'RequestId': '2cc8f8fe-86d0-49f3-b47b-5e5a2f7e46f7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '2cc8f8fe-86d0-49f3-b47b-5e5a2f7e46f7', 'content-type': 'text/xml', 'content-length': '2643', 'date': 'Mon, 18 Mar 2024 22:57:14 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "#create the redshift cluster\n",
    "try: \n",
    "        response = redshift.create_cluster(        \n",
    "            #HW\n",
    "            ClusterType=DWH_CLUSTER_TYPE,\n",
    "            NodeType=DWH_NODE_TYPE,\n",
    "            NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "            #Identifiers & Credentials\n",
    "            DBName=DWH_DB,\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "            MasterUsername=DWH_DB_USER,\n",
    "            MasterUserPassword=DWH_DB_PASSWORD,\n",
    "            \n",
    "            #Roles (for s3 access)\n",
    "            IamRoles=[roleArn]  \n",
    "        )\n",
    "        print(response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to redshift cluster using redshift_connector\n",
    "conn = redshift_connector.connect(\n",
    "    host=\"covid-redshift-cluster.cw6wwmbewavp.us-east-1.redshift.amazonaws.com\",\n",
    "    database='covid-db',\n",
    "    user='awsuser',\n",
    "    password='Passw0rd123'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the cursor\n",
    "cur = redshift_connector.Cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x14b1091b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(fact_final_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"dim_region_final\" (\n",
      "\"index\" TEXT,\n",
      "  \"1\" INTEGER\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dim_region_final_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x14b1091b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the tables in redshift\n",
    "cur.execute(dim_region_final_sql)\n",
    "cur.execute(dim_date_sql)\n",
    "cur.execute(dim_hospital_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x14b1091b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#copy command to copy the data from s3 to redshift\n",
    "cur.execute(\"\"\"\n",
    "            copy fact_final from 's3://akash-covid-project-1/output/fact_final.csv' \n",
    "            credentials 'aws_iam_role=arn:aws:iam::039330899250:role/redshift-s3-access' \n",
    "            delimiter ',' \n",
    "            region 'us-east-1' \n",
    "            ignoreheader 1\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "{'S': 'ERROR', 'C': 'XX000', 'M': \"Load into table 'dim_region_final' failed.  Check 'stl_load_errors' system table for details.\", 'F': '../src/pg/src/backend/commands/commands_copy.c', 'L': '737', 'R': 'CheckMaxRowError'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#similarly copy the other tables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m            copy dim_region_final from \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3://akash-covid-project-1/output/dim_region_final.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m            credentials \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maws_iam_role=arn:aws:iam::039330899250:role/redshift-s3-access\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m            delimiter \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m            region \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mus-east-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m            ignoreheader 1\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m            \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/redshift_connector/cursor.py:248\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, args, stream, merge_socket_read)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/redshift_connector/cursor.py:241\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, args, stream, merge_socket_read)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin transaction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39mmerge_socket_read \u001b[38;5;241m=\u001b[39m merge_socket_read\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/redshift_connector/core.py:1987\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, cursor, operation, vals)\u001b[0m\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_messages_merge_socket_read(cursor)\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1987\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/redshift_connector/core.py:2194\u001b[0m, in \u001b[0;36mConnection.handle_messages\u001b[0;34m(self, cursor)\u001b[0m\n\u001b[1;32m   2191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage_types[code](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read(data_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m), cursor)\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror\n",
      "\u001b[0;31mProgrammingError\u001b[0m: {'S': 'ERROR', 'C': 'XX000', 'M': \"Load into table 'dim_region_final' failed.  Check 'stl_load_errors' system table for details.\", 'F': '../src/pg/src/backend/commands/commands_copy.c', 'L': '737', 'R': 'CheckMaxRowError'}"
     ]
    }
   ],
   "source": [
    "#similarly copy the other tables\n",
    "cur.execute(\"\"\"\n",
    "            copy dim_region_final from 's3://akash-covid-project-1/output/dim_region_final.csv' \n",
    "            credentials 'aws_iam_role=arn:aws:iam::039330899250:role/redshift-s3-access' \n",
    "            delimiter ',' \n",
    "            region 'us-east-1' \n",
    "            ignoreheader 1\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'covid-redshift-cluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'awsuser',\n",
       "  'DBName': 'covid-db',\n",
       "  'Endpoint': {'Address': 'covid-redshift-cluster.cw6wwmbewavp.us-east-1.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2024, 3, 18, 22, 58, 23, 468000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0557e607d10acd930',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-030ca89833b123009',\n",
       "  'AvailabilityZone': 'us-east-1e',\n",
       "  'PreferredMaintenanceWindow': 'sun:06:30-sun:07:00',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 1,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::039330899250:role/redshift-s3-access',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2024, 3, 24, 6, 30, tzinfo=tzutc()),\n",
       "  'ClusterNamespaceArn': 'arn:aws:redshift:us-east-1:039330899250:namespace:ae489cde-7558-4560-b4bf-91707de9bec0',\n",
       "  'TotalStorageCapacityInMegaBytes': 400000,\n",
       "  'AquaConfiguration': {'AquaStatus': 'disabled',\n",
       "   'AquaConfigurationStatus': 'auto'},\n",
       "  'MultiAZ': 'Disabled'},\n",
       " 'ResponseMetadata': {'RequestId': 'fa002cb5-3188-473c-9175-eb924a28288b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fa002cb5-3188-473c-9175-eb924a28288b',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2921',\n",
       "   'date': 'Tue, 19 Mar 2024 01:30:19 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete the cluster\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete an ec2 instance. first list all the instances present\n",
    "instances = ec2.instances.filter(Filters=[{'Name': 'instance-state-name', 'Values': ['running','terminated']}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-0ec9f23d51e9f4e9b {'Code': 48, 'Name': 'terminated'}\n"
     ]
    }
   ],
   "source": [
    "for instance in instances:\n",
    "    print(instance.id, instance.state)\n",
    "    ec2.instances.filter(InstanceIds=[instance.id]).terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airflow-standalone password on EC2 \n",
    "wNzNuAvURRDfkmtY\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
